{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nwj_7xZ3C7CL",
    "outputId": "55aa53b3-2021-4469-d15f-ea495920a2be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pik8mQaPr8-w"
   },
   "outputs": [],
   "source": [
    "directory_name = ''\n",
    "filename = os.path.join(directory_name, 'ratings.csv')\n",
    "dataset = pd.read_csv(filename, sep=',')\n",
    "\n",
    "X_Data = np.asanyarray(dataset.sort_values(['userId', 'timestamp']).groupby('userId')['timestamp'].apply(list))\n",
    "X_data = dataset.sort_values(['userId', 'timestamp']).groupby('userId')['movieId'].apply(list)\n",
    "X_data = tf.keras.preprocessing.sequence.pad_sequences(X_data)\n",
    "\n",
    "l = []\n",
    "for i in range(1, len(X_data)):\n",
    "    l1 = []\n",
    "    for e in X_data[i]:\n",
    "        l1.append(str(e))\n",
    "    l.append(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 30)          6275130   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               81408     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 30)                3870      \n",
      "_________________________________________________________________\n",
      "reverse_embedding (ReverseEm (None,)                   0         \n",
      "=================================================================\n",
      "Total params: 6,360,408\n",
      "Trainable params: 6,360,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "class CustomLoss(tf.keras.losses.CosineSimilarity):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        return super().call(model.layers[0](y_true), model.layers[0](y_pred))\n",
    "\n",
    "class ReverseEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(ReverseEmbedding, self).__init__()\n",
    "        \n",
    "    def get_config(self):\n",
    "        return {}\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        dot_product = tf.matmul(\n",
    "            tf.nn.l2_normalize(model.layers[0].weights[0], axis=1),\n",
    "            tf.nn.l2_normalize(inputs, axis=1),\n",
    "            transpose_b=True)\n",
    "        return tf.argmax(dot_product,axis=0)\n",
    "\n",
    "vocab_size=100\n",
    "vector_size=30\n",
    "max_val = 209171\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(max_val, vector_size, mask_zero=True))\n",
    "model.add(tf.keras.layers.LSTM(128))\n",
    "model.add(tf.keras.layers.Dense(vector_size, activation='linear'))\n",
    "model.add(ReverseEmbedding())\n",
    "loss = CustomLoss()\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['lstm/lstm_cell/kernel:0', 'lstm/lstm_cell/recurrent_kernel:0', 'lstm/lstm_cell/bias:0', 'dense/kernel:0', 'dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['lstm/lstm_cell/kernel:0', 'lstm/lstm_cell/recurrent_kernel:0', 'lstm/lstm_cell/bias:0', 'dense/kernel:0', 'dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['lstm/lstm_cell/kernel:0', 'lstm/lstm_cell/recurrent_kernel:0', 'lstm/lstm_cell/bias:0', 'dense/kernel:0', 'dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['lstm/lstm_cell/kernel:0', 'lstm/lstm_cell/recurrent_kernel:0', 'lstm/lstm_cell/bias:0', 'dense/kernel:0', 'dense/bias:0'] when minimizing the loss.\n",
      "3/3 [==============================] - 3s 1s/step - loss: 9.5154e-04 - val_loss: -2.1153e-04\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# train_x = np.random.randint(1000, size=(100,3014))\n",
    "# train_y = np.random.randint(1000, size=(100,1))\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('keras_model.hdf5')\n",
    "history = model.fit(train_x, train_y, epochs=1, batch_size=32, validation_split=0.2, callbacks=[checkpoint]\n",
    "                   , workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['lstm/lstm_cell/kernel:0', 'lstm/lstm_cell/recurrent_kernel:0', 'lstm/lstm_cell/bias:0', 'dense/kernel:0', 'dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['lstm/lstm_cell/kernel:0', 'lstm/lstm_cell/recurrent_kernel:0', 'lstm/lstm_cell/bias:0', 'dense/kernel:0', 'dense/bias:0'] when minimizing the loss.\n",
      "1/1 [==============================] - 0s 839us/step - loss: 0.0314\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['lstm/lstm_cell/kernel:0', 'lstm/lstm_cell/recurrent_kernel:0', 'lstm/lstm_cell/bias:0', 'dense/kernel:0', 'dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['lstm/lstm_cell/kernel:0', 'lstm/lstm_cell/recurrent_kernel:0', 'lstm/lstm_cell/bias:0', 'dense/kernel:0', 'dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_train_function.<locals>.train_function at 0x7f7bc8111160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n",
      "1/1 [==============================] - 0s 881us/step - loss: -0.0388\n"
     ]
    }
   ],
   "source": [
    "# model.fit(, , epochs=1, batch_size=1)\n",
    "l = [[132, 213, 4234, 4342, 3223, 32], [32, 421, 21,432]]\n",
    "for i in l:\n",
    "    model.fit([i[:-1]], [l[-1]])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
